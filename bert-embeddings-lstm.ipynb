{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nimport numpy as np","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\nmodel_bert = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807d6ebfa7df426a93cee98849f36a47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46665c69218c4bfd898ec0b4f359aba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d41c23306224125af9ac4b8252d3ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b36f0e0d89442cdbb35e8a8e70494fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4fac1b14fc04c428d569c36ac6c6337"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/unsafe/train_randst0.csv')\ntest_df = pd.read_csv('/kaggle/input/unsafe/val_randst0.csv')\ntrain_df = train_df[[\"text\", \"unsafe\"]]\ntest_df = test_df[[\"text\", \"unsafe\"]]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.loc[(train_df['unsafe'] >= 0.8) | (train_df['unsafe'] <= 0.2)]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary(prob):\n    if prob < 0.5:\n        return 0.\n    else:\n        return 1.0","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['unsafe'] = train_df['unsafe'].apply(binary)\ntest_df['unsafe'] = test_df['unsafe'].apply(binary)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(                                                     text  unsafe\n 0       я думал что левиафаны - это те медленные страх...     1.0\n 1       А был бы этот полицейский в Петербурге, так пе...     1.0\n 2       Напоминаю, что пора искать актис невзрослого п...     1.0\n 3       курю лет пятнадцать никаких проблем кроме како...     1.0\n 4       окей, я тогда проведу парад гетеросексуалов, п...     1.0\n ...                                                   ...     ...\n 138825      Перед клиентом отвечает банк, а не сотрудник.     0.0\n 138826  Так воооот откуда я их знаю, какое старое виде...     0.0\n 138827  Да потом просто \"такие вот люди\" начинают жало...     0.0\n 138829  А теперь давай фоточки не миллионеров, а обычн...     0.0\n 138830  Не нашел информации о том, что он был доктором...     0.0\n \n [120120 rows x 2 columns],\n                                                     text  unsafe\n 0                       уровень ссачнее, чем ад в доом 3     1.0\n 1      У нас несколько спортсменов и спортсменок в сп...     1.0\n 2      Я уж думал нахлебались праздников Запада ...а ...     1.0\n 3      Лол, большая часть около NUMBER% и так на нога...     1.0\n 4      Не, ну тут действительно в какой то степени ма...     1.0\n ...                                                  ...     ...\n 24496                Почему все так вокруг Киану пляшут?     0.0\n 24497  Проблема то не в том, что ты деньги проиграл. ...     0.0\n 24498  Не не не, не говори. Пусть остается в блаженно...     0.0\n 24499  На иврите будет אין חניה  , а не то что там на...     0.0\n 24500  Директор может предоставить беспроцентный заем...     0.0\n \n [24501 rows x 2 columns])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_df['text'].tolist()\ny_train = train_df['unsafe'].tolist()\nx_test = test_df['text'].tolist()\ny_test = test_df['unsafe'].tolist()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 45","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UnsafeData(Dataset):\n\n    def __init__(self, texts, targets, tokenizer, max_len):\n        \n        super().__init__()\n        \n        self.texts = texts\n        self.targets = targets        \n        self.max_len = max_len\n        \n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        \n        return len(self.texts)\n\n    \n    def __getitem__(self, index):\n        x = self.texts[index]\n        y = self.targets[index]\n        \n        enc_dict = self.tokenizer(x, truncation=True, max_length=self.max_len, padding='max_length')\n        \n        tokenized = enc_dict['input_ids']\n        mask = enc_dict['attention_mask']\n        \n        x = torch.tensor(tokenized).long()\n        mask = torch.tensor(mask).long()\n        y = torch.tensor(y).float()\n        \n        return x, mask, y","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = UnsafeData(x_train, y_train, tokenizer, max_len)\ntest_dataset = UnsafeData(x_test, y_test, tokenizer, max_len)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataset), len(test_dataset)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(120120, 24501)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[23000], y_train[23000]","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"('не расстраивайтесь, что его быстро отпустят. Главное, что он в отделение попал, а там уже и героин в чемодане найдётся и какая-нибудь порнография и экстремистские репосты в телефоне',\n 1.0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[23000]","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(tensor([   101,   1699, 108867,  11213,  77489,    128,   1997,   2752,  13586,\n          26375,   2190,    132,  27609,    128,   1997,   2886,    845,  15636,\n          15380,    128,    625,   8528,   4745,    851,  87943,    845,  81741,\n            842,   1469,  10596,   1523,    851,  36014,    130,  22655,  36522,\n           1577,    851, 114928,  12710, 107485,    845, 111152,    102,      0]),\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]),\n tensor(1.))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle = True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle = True)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, m, y in train_loader:\n    break\n\nx.shape, m.shape, y.shape","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"(torch.Size([64, 45]), torch.Size([64, 45]), torch.Size([64]))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Новая модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RCNN(nn.Module):\n\n    \n    def __init__(self, embedding_dim, hidden_size, hidden_size_linear, class_num, dropout, n_layers):\n        super(RCNN, self).__init__()\n        self.pretrained_model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True, dropout=dropout, num_layers=n_layers)\n        self.W = nn.Linear(embedding_dim + 2*hidden_size, hidden_size_linear)\n        self.fc = nn.Linear(hidden_size_linear, class_num)\n        \n        self.act = nn.Sigmoid()\n        \n        for param in self.pretrained_model.parameters():\n                param.requires_grad = False\n\n        \n    def forward(self, x, mask):\n        # x = |bs, seq_len|\n        x_emb = self.pretrained_model(x, mask)[0]\n        # x_emb = |bs, seq_len, embedding_dim|\n        output, _ = self.lstm(x_emb)\n        # output = |bs, seq_len, 2*hidden_size|\n        output = torch.cat([output, x_emb], 2)\n        # output = |bs, seq_len, embedding_dim + 2*hidden_size|\n        output = self.W(output).transpose(1, 2)\n        # output = |bs, seq_len, hidden_size_linear| -> |bs, hidden_size_linear, seq_len|\n        output = F.max_pool1d(output, output.size(2)).squeeze(2)\n        # output = |bs, hidden_size_linear|\n        output = self.fc(output)\n        # output = |bs, class_num|\n        return self.act(output)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RCNN(\n    embedding_dim = 768,\n    hidden_size = 256,\n    hidden_size_linear = 128,\n    class_num = 1,\n    n_layers = 3,\n    dropout = 0.5\n)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(x, m).size()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"torch.Size([64, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda')\n#device = torch.device('cpu')\nmodel.to(device)","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"RCNN(\n  (pretrained_model): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (lstm): LSTM(768, 256, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n  (W): Linear(in_features=1280, out_features=128, bias=True)\n  (fc): Linear(in_features=128, out_features=1, bias=True)\n  (act): Sigmoid()\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 7\nlearning_rate = 0.0001\nwarmup_steps = 50\ntotal_steps = len(train_loader) * num_epochs","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=warmup_steps,\n                                            num_training_steps=total_steps)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCELoss()","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metrics(true, predictions):\n    \n    rounded_preds = torch.round(predictions)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(true, rounded_preds, average='binary', zero_division = 0)\n    acc = accuracy_score(true, rounded_preds)\n    #roc_auc = roc_auc_score(true, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n        #'roc_auc': roc_auc\n    }   \n    ","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, loader, optimizer, scheduler, criterion, last_n_losses=200, verbose=True):\n\n    losses = []\n    f_scores = []\n    accuracy_scores = []\n    precision_scores = []\n    recall_scores = []\n    #roc_auc_scores = []\n\n    progress_bar = tqdm(total=len(loader), disable=not verbose, desc='Train')\n\n    model.train()\n\n    for x, m, y in loader:\n\n        x = x.to(device)\n        m = m.to(device)\n        y = y.to(device)\n        \n        optimizer.zero_grad()\n        \n        yhat = model(x, m).squeeze()\n        \n        loss = criterion(yhat, y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        cur_metrics = metrics(y.cpu(), yhat.detach().cpu())\n\n\n        losses.append(loss.item())\n        f_scores.append(cur_metrics['f1'])\n        accuracy_scores.append(cur_metrics['accuracy'])\n        precision_scores.append(cur_metrics['precision'])\n        recall_scores.append(cur_metrics['recall'])\n        #roc_auc_scores.append(cur_metrics['roc_auc'])\n        \n\n        progress_bar.set_postfix(loss=np.mean(losses[-last_n_losses:]), f1=np.mean(f_scores[-last_n_losses:]),\n                                accuracy=np.mean(accuracy_scores[-last_n_losses:]))\n        \n        progress_bar.update()\n\n    progress_bar.close()\n    \n    return {'loss': np.sum(losses)/len(loader), 'f_score': np.sum(f_scores)/len(loader), 'accuracy': np.sum(accuracy_scores)/len(loader),\n           'precision': np.sum(precision_scores)/len(loader), 'recall': np.sum(recall_scores)/len(loader)}","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, loader, criterion, last_n_losses=200, verbose=True):\n\n    losses = []\n    f_scores = []\n    accuracy_scores = []\n    precision_scores = []\n    recall_scores = []\n    #roc_auc_scores = []\n\n    progress_bar = tqdm(total=len(loader), disable=not verbose, desc='Eval')\n\n    model.eval()\n    with torch.no_grad():\n\n        for x, m, y in loader:\n\n            x = x.to(device)\n            m = m.to(device)\n            y = y.to(device)\n        \n            yhat = model(x, m).squeeze()\n        \n            loss = criterion(yhat, y)\n        \n            cur_metrics = metrics(y.cpu(), yhat.detach().cpu())\n\n\n            losses.append(loss.item())\n            f_scores.append(cur_metrics['f1'])\n            accuracy_scores.append(cur_metrics['accuracy'])\n            precision_scores.append(cur_metrics['precision'])\n            recall_scores.append(cur_metrics['recall'])\n            #roc_auc_scores.append(cur_metrics['roc_auc'])\n        \n\n            progress_bar.set_postfix(loss=np.mean(losses[-last_n_losses:]), f1=np.mean(f_scores[-last_n_losses:]),\n                                accuracy=np.mean(accuracy_scores[-last_n_losses:]))\n        \n            progress_bar.update()\n\n        progress_bar.close()\n    \n    return {'loss': np.sum(losses)/len(loader), 'f_score': np.sum(f_scores)/len(loader), 'accuracy': np.sum(accuracy_scores)/len(loader),\n           'precision': np.sum(precision_scores)/len(loader), 'recall': np.sum(recall_scores)/len(loader)}","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tqdm._instances.clear()","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_best_model_path = '/kaggle/working/best_model_state_dict.pth'\nsave_best_optimizer_path = '/kaggle/working/best_optimizer_state_dict.pth'","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 7\nbest_valid_loss = float('inf')\npatience = 0\nfor epoch in range(n_epochs):\n     \n    #train the model\n    train_metrics = train(model, train_loader, optimizer, scheduler, criterion)\n    \n    #evaluate the model\n    valid_metrics = evaluate(model, test_loader, criterion)\n    \n    print(train_metrics)\n    print(valid_metrics)\n    \n    #save the best model\n    if valid_metrics['loss'] < best_valid_loss:\n        best_valid_loss = valid_metrics['loss']\n        torch.save(model.state_dict(), save_best_model_path)\n        torch.save(optimizer.state_dict(), save_best_optimizer_path)\n    else:\n        patience +=1\n        if patience>3:\n            break","execution_count":58,"outputs":[{"output_type":"stream","text":"Train: 100%|██████████| 1877/1877 [04:53<00:00,  6.39it/s, accuracy=0.772, f1=0.481, loss=0.472]\nEval: 100%|██████████| 383/383 [00:47<00:00,  8.07it/s, accuracy=0.751, f1=0.509, loss=0.504]\n","name":"stderr"},{"output_type":"stream","text":"{'loss': 0.48951109199048864, 'f_score': 0.4387170341768001, 'accuracy': 0.7606184355734834, 'precision': 0.6072930644693312, 'recall': 0.3642996738627255}\n{'loss': 0.504410288352568, 'f_score': 0.5067518978830489, 'accuracy': 0.7474013498201882, 'precision': 0.6081783674765513, 'recall': 0.4454706205444552}\n","name":"stdout"},{"output_type":"stream","text":"Train:  22%|██▏       | 407/1877 [08:34<30:57,  1.26s/it, accuracy=0.755, f1=0.44, loss=0.496]]\nTrain: 100%|██████████| 1877/1877 [04:55<00:00,  6.35it/s, accuracy=0.789, f1=0.55, loss=0.44]  \nEval: 100%|██████████| 383/383 [00:47<00:00,  8.06it/s, accuracy=0.757, f1=0.508, loss=0.498]\n","name":"stderr"},{"output_type":"stream","text":"{'loss': 0.4573057105106817, 'f_score': 0.5160473756741593, 'accuracy': 0.7799930074587107, 'precision': 0.6613117393834752, 'recall': 0.4409182372835891}\n{'loss': 0.5022160625644515, 'f_score': 0.5096757546461277, 'accuracy': 0.754579966993448, 'precision': 0.6357995072179342, 'recall': 0.4357155363031775}\n","name":"stdout"},{"output_type":"stream","text":"Train: 100%|██████████| 1877/1877 [04:56<00:00,  6.34it/s, accuracy=0.79, f1=0.567, loss=0.436] \nEval: 100%|██████████| 383/383 [00:47<00:00,  8.03it/s, accuracy=0.759, f1=0.514, loss=0.495]\n","name":"stderr"},{"output_type":"stream","text":"{'loss': 0.43944619017264663, 'f_score': 0.552622187859983, 'accuracy': 0.791391563284877, 'precision': 0.6778449824295868, 'recall': 0.4813499733899634}\n{'loss': 0.49438481480272256, 'f_score': 0.5180041938122543, 'accuracy': 0.7599581568057541, 'precision': 0.6550837232305435, 'recall': 0.4397148889890095}\n","name":"stdout"},{"output_type":"stream","text":"Train: 100%|██████████| 1877/1877 [04:56<00:00,  6.34it/s, accuracy=0.796, f1=0.577, loss=0.424]\nEval: 100%|██████████| 383/383 [00:47<00:00,  8.03it/s, accuracy=0.758, f1=0.513, loss=0.489]\n","name":"stderr"},{"output_type":"stream","text":"{'loss': 0.42593506083376687, 'f_score': 0.5761581310447734, 'accuracy': 0.7988788149783089, 'precision': 0.6886865492445897, 'recall': 0.5117209774554276}\n{'loss': 0.4903745595219864, 'f_score': 0.5140265404131781, 'accuracy': 0.7605701019754668, 'precision': 0.6523443471658726, 'recall': 0.43505871898228626}\n","name":"stdout"},{"output_type":"stream","text":"Train: 100%|██████████| 1877/1877 [04:56<00:00,  6.33it/s, accuracy=0.809, f1=0.602, loss=0.41] \nEval: 100%|██████████| 383/383 [00:47<00:00,  8.00it/s, accuracy=0.77, f1=0.56, loss=0.487]  \nTrain:   0%|          | 1/1877 [00:00<05:08,  6.07it/s, accuracy=0.875, f1=0.636, loss=0.346]","name":"stderr"},{"output_type":"stream","text":"{'loss': 0.41400120004447905, 'f_score': 0.5989752783694108, 'accuracy': 0.8078311705609255, 'precision': 0.7031250565166668, 'recall': 0.5361469069674489}\n{'loss': 0.49416776456658584, 'f_score': 0.560535533270204, 'accuracy': 0.7655487955071678, 'precision': 0.6400814983752748, 'recall': 0.5123898497712712}\n","name":"stdout"},{"output_type":"stream","text":"Train: 100%|██████████| 1877/1877 [04:56<00:00,  6.33it/s, accuracy=0.815, f1=0.618, loss=0.406]\nEval: 100%|██████████| 383/383 [00:47<00:00,  8.05it/s, accuracy=0.762, f1=0.528, loss=0.504]\nTrain:   0%|          | 1/1877 [00:00<05:12,  6.01it/s, accuracy=0.812, f1=0.647, loss=0.367]","name":"stderr"},{"output_type":"stream","text":"{'loss': 0.4047103945362358, 'f_score': 0.6132111585886557, 'accuracy': 0.8119886406880279, 'precision': 0.7075688442623046, 'recall': 0.5549054505440082}\n{'loss': 0.5038097676658132, 'f_score': 0.529398676561949, 'accuracy': 0.7635582417853096, 'precision': 0.6593071762448177, 'recall': 0.4530765881481388}\n","name":"stdout"},{"output_type":"stream","text":"Train:   3%|▎         | 60/1877 [00:09<04:43,  6.40it/s, accuracy=0.826, f1=0.636, loss=0.385]","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-238b9ad2e8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-55-143a9e24d137>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, scheduler, criterion, last_n_losses, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcur_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-084f4096289c>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(true, predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrounded_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#roc_auc = roc_auc_score(true, predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     MCM = multilabel_confusion_matrix(y_true, y_pred,\n\u001b[1;32m   1467\u001b[0m                                       \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m                                       labels=labels, samplewise=samplewise)\n\u001b[0m\u001b[1;32m   1469\u001b[0m     \u001b[0mtp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0mpred_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36m_unique_multiclass\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_unique_multiclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munique\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignoring\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \"\"\"\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0moptional_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}